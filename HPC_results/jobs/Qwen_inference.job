#!/bin/bash

#SBATCH --job-name=qw_inference        	# Job name
#SBATCH --output=qw_inference.%j.out    	# Name of output file (%j expands to jobId)
#SBATCH --error=qw_inference.%j.err     	# Error file
#SBATCH --cpus-per-task=8        # Schedule two cores
#SBATCH --mem=64G
#SBATCH --gres=gpu:v100:1               # Schedule a GPU, or more with gpu:2 etc
#SBATCH --time=72:00:00          # Run time (hh:mm:ss)
#SBATCH --partition=acltr   # Run on any cores queue
#SBATCH --mail-type=FAIL          # Send an email when the job finishes

#Load CUDA
module load CUDA/12.1.1

# Load python
module load Python/3.12.3-GCCcore-13.3.0
python --version

# Print out the hostname of the node the job is running on
hostname
python -c "import torch; print('Torch:', torch.__version__)"
python -c "import torch; print(torch.cuda.is_available(), torch.cuda.get_device_name(0))"

#activate venv
source ~/envs/torch121_gpu/bin/activate

# Run scripts
python ../scripts/qwen_inference.py