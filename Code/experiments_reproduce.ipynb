{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57b4962",
   "metadata": {},
   "source": [
    "# Experiments \n",
    "\n",
    "Challenge 3: Memorization of outdated medical knowledge in LLMs\n",
    "Group: C\n",
    "\n",
    "Paper: Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge in Large Language Models\n",
    "Reproducibility task:\n",
    "\n",
    "Read the paper: https://arxiv.org/pdf/2509.04304\n",
    "\n",
    "Reproduce parts of their experiments investigating when LLMs predict outdated medical knowledge using this repository: https://github.com/jvladika/MedChange\n",
    "\n",
    "Reproducibility target: Your goal is to reproduce parts of the result in Table 2. Specifically, you **evaluate the Qwen 2.5 7B and the BioMistral 7B model** on the **Changed Knowledge Dataset**. Your task is to reproduce the precision, recall F1colums for (b) outdated Lab. and (c) Latest Labels, as well as the F1 diff colum for those two LLMs.\n",
    "\n",
    "Notes from TA:\n",
    "✅ Code and datasets available in the repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaea5d0e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12fbecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4856534",
   "metadata": {},
   "source": [
    "## Set up dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9051852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scraped studies' data and extract all DOIs from the date string\n",
    "# Studies with multiple iterations have the same DOI, but end with \"v2\", \"v3\", etc.\n",
    "\n",
    "df = pd.read_csv(\"./FullScraped.csv\", index_col=0)\n",
    "dois = [d.split(\"doi: \")[1].strip() if \"doi: \" in d else \"\" for d in df.date.tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de0c37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the generated questions and labels from the text fiel\n",
    "with open(\"generated_questions_labels.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "questions = list()\n",
    "labels = list()\n",
    "\n",
    "for idx,line in enumerate(lines):\n",
    "    #if idx in missing_indices:\n",
    "    #    continue\n",
    "    line = line.split(\"\\n\")[0]\n",
    "    line = line.split(\" ||| \")\n",
    "    questions.append(line[0].split(\"QUESTION: \")[1].strip())\n",
    "    labels.append(line[1].split(\"LABEL: \")[1].strip())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c817793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the questions by DOI, so that multiple iterations of the same study are together in a group\n",
    "doigroups = list()\n",
    "indices = {}\n",
    "for idx in range(len(dois)):\n",
    "    d = dois[idx]\n",
    "    if d not in indices:\n",
    "        indices[d] = idx \n",
    "\n",
    "doisorted = sorted(dois)\n",
    "\n",
    "# Iterate through the sorted DOIs and group them by common prefix. \"Pivot\" is the earliest study in the group.\n",
    "idx = 0\n",
    "while idx < len(dois):\n",
    "    pivot = doisorted[idx]\n",
    "    if pivot == \"\":\n",
    "        idx += 1\n",
    "        continue\n",
    "    doigroup = list()\n",
    "    while idx < len(dois) and doisorted[idx].startswith(pivot):\n",
    "        doigroup.append(indices[doisorted[idx]])\n",
    "        idx += 1\n",
    "    doigroups.append(doigroup)\n",
    "\n",
    "doigroups = [list(set(dg)) for dg in doigroups]\n",
    "doigroups = sorted(doigroups)\n",
    "\n",
    "def remove_duplicates(lst):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for item in lst:\n",
    "        if item not in seen:\n",
    "            seen.add(item)\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "doigroups = [remove_duplicates(dg) for dg in doigroups]\n",
    "doigroups = sorted(doigroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f51d3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1535\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping of study iterations (by index) to their labels, for studies with multiple iterations\n",
    "long_doigroups = list()\n",
    "long_indices = list()\n",
    "for idx,dg in enumerate(doigroups):\n",
    "    if len(dg) > 1:\n",
    "        long_doigroups.append(dg)\n",
    "        long_indices.append(idx)\n",
    "\n",
    "verdict_map = list()\n",
    "for ld in long_doigroups:\n",
    "    instance_verdicts = {}\n",
    "    sorted_ld = sorted(ld)\n",
    "    for idx in sorted_ld:\n",
    "        instance_verdicts[idx] = labels[idx]\n",
    "    verdict_map.append(instance_verdicts)\n",
    "\n",
    "print(len(verdict_map))\n",
    "#verdict_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7701f2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generated answers of all models.\n",
    "\n",
    "filenames = [#'llama33-70b_answers.txt',\n",
    " #'mistral-24b_answers.txt',\n",
    " #'gpt4o-mini_answers.txt',\n",
    " 'qwen25-7b_answers.txt',\n",
    " #'deepsek-v3_answers.txt',\n",
    " #'olmo_gguf_answers_13b.txt',\n",
    " #'pmcllama_answers.txt',\n",
    " #'biomistral_answers.txt', \n",
    " #'biomistral_answers_repro.txt', \n",
    " 'qwen_answers_repro.txt'\n",
    " ]\n",
    "\n",
    "filenames = [\"GeneratedAnswers/\"+f for f in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "535dc431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneratedAnswers/qwen25-7b_answers.txt\n",
      "GeneratedAnswers/qwen_answers_repro.txt\n"
     ]
    }
   ],
   "source": [
    "for file in filenames:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df44351c",
   "metadata": {},
   "source": [
    "## Get MedChangeQA results (change knowledge subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "051d2908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1535, 512)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get all the labels of MedChangeQA\n",
    "\n",
    "final_labels = list()\n",
    "final_keys = list()\n",
    "outdated_keys = list()\n",
    "outdated_labels = list()  \n",
    "newest_labels = list()   \n",
    "newest_keys = list()\n",
    "\n",
    "for verdict_dict in verdict_map:\n",
    "    final_label = verdict_dict[list(verdict_dict.keys())[0]]\n",
    "    final_labels.append(final_label)\n",
    "    final_keys.append(list(verdict_dict.keys())[0])\n",
    "\n",
    "    verdict_changed = False\n",
    "    for key in verdict_dict.keys():\n",
    "        if verdict_dict[key] != final_label:\n",
    "\n",
    "            outdated_keys.append(key)\n",
    "            outdated_labels.append(verdict_dict[key])\n",
    "\n",
    "            newest_labels.append(verdict_dict[list(verdict_dict.keys())[0]])\n",
    "            newest_keys.append(list(verdict_dict.keys())[0])\n",
    "\n",
    "            verdict_changed = True\n",
    "            break\n",
    "\n",
    "# MedChangeQA is a subset where 512 questions changed their verdict over time\n",
    "len(final_labels), len(newest_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a94bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Get final evaluation scores of the predicted LLM answers\n",
    "\n",
    "def get_predictions_change_knowledge(filename, label_type):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    #if \"biomistral\" in filename or \"pmcllama\" in filename:\n",
    "        #print(\"biomistral or pcmllama\")\n",
    "        #lines = lines[1::3]\n",
    "    \n",
    "    # Get all the predicted labels on the entire dataset MedRevQA\n",
    "    llm_predicted_labels = list()\n",
    "    tokens_supported = (\"SUPPORTED\", \"Supported:\", \"EFFECTIVE\",\n",
    "                        \": Supported\",\"(supported)\", \"(Supported)\", \"Effective:\",\n",
    "                         \": YES\", \"YES:\", \"POSITIVE\", \"(YES)\", \"SUPPORTed\")\n",
    "    \n",
    "    tokens_nei =(\"NOT ENOUGH INFORMATION:\", \"NOT ENOUGH INFORMATION\", \"NOT_ENOUGH_INFORMATION\"\n",
    "                \"Not Enough Information:\", \"Not enough information.\" )\n",
    "\n",
    "\n",
    "    for line in lines:\n",
    "        #line = re.sub(r'[^\\w\\s]','',line_notnorm).strip()\n",
    "        \n",
    "        if \"REFUTED\" in line or \"REF UTED\":\n",
    "            llm_predicted_labels.append(\"REFUTED\")\n",
    "        elif any(tok in line for tok in tokens_nei):\n",
    "            llm_predicted_labels.append(\"NOT ENOUGH INFORMATION\")\n",
    "        elif any(tok in line for tok in tokens_supported):\n",
    "            llm_predicted_labels.append(\"SUPPORTED\")\n",
    "        else:\n",
    "            llm_predicted_labels.append(\"NO LABEL FOUND\")\n",
    "            #print(line)\n",
    "            #print(line_notnorm)\n",
    "            #print('.....')\n",
    "        \n",
    "    print(f'len llm predicted labels: {len(llm_predicted_labels)}')\n",
    "    print(f'len \"NO LABEL FOUND\": {llm_predicted_labels.count('NO LABEL FOUND')}')\n",
    "\n",
    "\n",
    "    # Get only the changed-knowledge subset (MedChangeQA)\n",
    "    #llm_predicted_medchange = np.array(llm_predicted_labels)[np.array(outdated_keys)]\n",
    "    #llm_predicted_medchange = np.array(llm_predicted_labels, dtype=object)[np.array(outdated_keys, dtype=int)]\n",
    "\n",
    "    mapper = {\"SUPPORTED\": 0, \"REFUTED\": 2, \"NOT ENOUGH INFORMATION\": 1}\n",
    "        \n",
    "    pred_all = np.array(llm_predicted_labels, dtype=object)[np.array(outdated_keys, dtype=int)]\n",
    "\n",
    "    gold_all = np.array([mapper[g] for g in (outdated_labels if label_type==\"OUTDATED\" else newest_labels)], dtype=int)\n",
    "\n",
    "    print(f'len(pred_all): {len(pred_all)}')\n",
    "    print(f'len(gold_all): {len(gold_all)}')\n",
    "\n",
    "\n",
    "    common_len = min(len(pred_all), len(gold_all))\n",
    "    if len(pred_all) != len(gold_all):\n",
    "        print(f\"[LENGTH MISMATCH] pred={len(pred_all)} vs gold={len(gold_all)} → {common_len}\")\n",
    "    pred_slice = pred_all[:common_len]\n",
    "    gold_slice = gold_all[:common_len]\n",
    "\n",
    "    #  Mask  (discart 'NO LABEL FOUND')\n",
    "    mask = (pred_slice != \"NO LABEL FOUND\")\n",
    "    dropped = int((~mask).sum())\n",
    "    if dropped:\n",
    "        masked_pos = np.flatnonzero(~mask)\n",
    "\n",
    "        subset_keys = np.array(outdated_keys, dtype=int)[:common_len]\n",
    "        masked_items = subset_keys[masked_pos]\n",
    "\n",
    "        is_triplet_used = (\"biomistral\" in filename) or (\"pmcllama\" in filename)\n",
    "\n",
    "        print(f\"[NO LABEL FOUND] {dropped} points masked. Showing up to 50:\")\n",
    "        for i, pos in enumerate(masked_pos[:50], start=1):\n",
    "            item_idx = int(masked_items[i-1])  \n",
    "            if 0 <= item_idx < len(lines):\n",
    "                preview = lines[item_idx].rstrip(\"\\n\")\n",
    "            else:\n",
    "                preview = \"<out of range in current 'lines' view>\"\n",
    "\n",
    "            if is_triplet_used:\n",
    "                file_line = item_idx * 3 + 2  \n",
    "            else:\n",
    "                file_line = item_idx + 1\n",
    "\n",
    "            if len(preview) > 220:\n",
    "                preview = preview[:220] + \"…\"\n",
    "            print(f\"  - subset_pos={int(pos)} | item_idx={item_idx} | file_line={file_line} | {preview}\")\n",
    "\n",
    "    \n",
    "    y_test = gold_slice[mask]    \n",
    "    y_pred = np.array([mapper[p] for p in pred_slice[mask]], dtype=int)\n",
    "\n",
    "    # Calculate precision, recall, and F1 score with macro averaging\n",
    "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    ## get precision, recall, f1, accuracy rounded to 4 decimal places\n",
    "\n",
    "    print(f\"Precision: {precision_macro:.4f} | Recall: {recall_macro:.4f} | F1: {f1_macro:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "    #MIRIAM ADDED THIS RETURN \n",
    "    return f1_macro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82480bad",
   "metadata": {},
   "source": [
    "FOR PMC\n",
    "Initially without normalization we have NO label found -> 391\n",
    "When we remove punctuation and white spaces -> 391 -> not worthy\n",
    "Include \"Supported\" -> 303\n",
    "Include \"EFFECTIVE\" and \"POSITIVE\" -> 181\n",
    "Include all in the tockens array -> 129\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc97de7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "GeneratedAnswers/qwen25-7b_answers.txt\n",
      "-----\n",
      " Latest\n",
      "len llm predicted labels: 16501\n",
      "len \"NO LABEL FOUND\": 0\n",
      "len(pred_all): 512\n",
      "len(gold_all): 512\n",
      "Precision: 0.0853 | Recall: 0.3333 | F1: 0.1358\n",
      "Accuracy: 0.2559\n",
      " Outdated\n",
      "len llm predicted labels: 16501\n",
      "len \"NO LABEL FOUND\": 0\n",
      "len(pred_all): 512\n",
      "len(gold_all): 512\n",
      "Precision: 0.0801 | Recall: 0.3333 | F1: 0.1291\n",
      "Accuracy: 0.2402\n",
      "\n",
      "F1 diff: 0.6687810990966714\n",
      "\n",
      "-----\n",
      "GeneratedAnswers/qwen_answers_repro.txt\n",
      "-----\n",
      " Latest\n",
      "len llm predicted labels: 16501\n",
      "len \"NO LABEL FOUND\": 0\n",
      "len(pred_all): 512\n",
      "len(gold_all): 512\n",
      "Precision: 0.0853 | Recall: 0.3333 | F1: 0.1358\n",
      "Accuracy: 0.2559\n",
      " Outdated\n",
      "len llm predicted labels: 16501\n",
      "len \"NO LABEL FOUND\": 0\n",
      "len(pred_all): 512\n",
      "len(gold_all): 512\n",
      "Precision: 0.0801 | Recall: 0.3333 | F1: 0.1291\n",
      "Accuracy: 0.2402\n",
      "\n",
      "F1 diff: 0.6687810990966714\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mespi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\mespi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\mespi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\mespi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "for file in filenames: \n",
    "    print('-----')\n",
    "    print(file)\n",
    "    print('-----')\n",
    "    print(\" Latest\")\n",
    "    f1_latest = get_predictions_change_knowledge(file, label_type='NEWEST')\n",
    "    print(\" Outdated\")\n",
    "    f1_outdated = get_predictions_change_knowledge(file, label_type='OUTDATED')\n",
    "    print('')\n",
    "    print(f\"F1 diff: {(f1_latest - f1_outdated)*100}\")\n",
    "    print('')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
