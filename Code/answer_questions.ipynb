{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c3fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f7b813",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load all the questions and labels\n",
    "\n",
    "with open(\"generated_questions_labels.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "questions = list()\n",
    "labels = list()\n",
    "levels = list()\n",
    "\n",
    "for line in lines:\n",
    "    line = line.split(\"\\n\")[0]\n",
    "    line = line.split(\" ||| \")\n",
    "    questions.append(line[0].split(\"QUESTION: \")[1].strip())\n",
    "    labels.append(line[1].split(\"LABEL: \")[1].strip())\n",
    "\n",
    "    # Levels of strength of evidence were additionally generated but were not used in further experiments.\n",
    "    levels.append(line[2].split(\"LEVEL: \")[1].strip())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4dcf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time \n",
    "\n",
    "## Prepare the messages for answering the questions by each LLM.\n",
    "## Most LLMs were called through Together AI API or hosted locally and queried.\n",
    "\n",
    "all_messages = list()\n",
    "for idx in range(len(questions)):\n",
    "\n",
    "    system_message = \"You're a helpful AI assistant helping answer clinical and medical questions based on your best knowledge.\"\n",
    "    user_message_begin = f'''\n",
    "           Please answer this clinical question only with SUPPORTED (if the question is supported by the clinical research) or REFUTED (if the hypothesis is refuted by the current clinical research) or NOT ENOUGH INFORMATION (if there is insufficient evidence for the question in current research). Please give your output in form of LABEL: (label) . Briefly explain your answer.\n",
    "\n",
    "    QUESTION:\n",
    "    '''\n",
    "\n",
    "    finalstring = \"\"\n",
    "    finalstring += user_message_begin\n",
    "    finalstring += questions[idx]  + \"\\n\"\n",
    "\n",
    "    messages =  [  \n",
    "            {'role':'system', \n",
    "             'content': system_message},    \n",
    "            {'role':'user', \n",
    "             'content': f\"{finalstring}\"},  \n",
    "    ] \n",
    "    \n",
    "    all_messages.append(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281e67f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "  api_key=\"xyz\",\n",
    "  base_url=\"https://api.together.xyz/v1\",\n",
    ")\n",
    "\n",
    "def query_llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\", # set model version\n",
    "        max_tokens = 512,\n",
    "        messages=prompt, # provide prompt in chat format\n",
    "        temperature=0) # set model temperature = 0\n",
    "    return response\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "with open(\"llama33-70b_answers.txt\", \"w\") as f:\n",
    "    curr_idx = 0\n",
    "\n",
    "    for msgs in all_messages:\n",
    "        response = query_llm(msgs)\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        result = result.replace(\"\\n\", \" ||| \")\n",
    "        f.write(result)\n",
    "        f.write(\"\\n\")\n",
    "        curr_idx += 1\n",
    "\n",
    "        # Track time.\n",
    "        if curr_idx % 1000 == 0:\n",
    "            end_time = time.time()\n",
    "            print(curr_idx, end_time-start_time)\n",
    "            start_time = time.time()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
